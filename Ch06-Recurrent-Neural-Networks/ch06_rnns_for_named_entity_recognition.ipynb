{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e4604b",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1556d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 54321\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa6421",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7ebbc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data\\conllpp_train.txt\n",
      "Found and verified data\\conllpp_dev.txt\n",
      "Found and verified data\\conllpp_test.txt\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/'\n",
    "dir_name = 'data'\n",
    "#https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_train.txt\n",
    "def download_data(url, filename, download_dir, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "      \n",
    "    # Create directories if doesn't exist\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    # If file doesn't exist download\n",
    "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "    \n",
    "    # Check the file size\n",
    "    statinfo = os.stat(filepath)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filepath)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filepath + '. Can you get to it with a browser?')\n",
    "        \n",
    "    return filepath\n",
    "\n",
    "# Filepaths to train/valid/test data\n",
    "train_filepath = download_data(url, 'conllpp_train.txt', dir_name, 3283420)\n",
    "dev_filepath = download_data(url, 'conllpp_dev.txt', dir_name, 827443)\n",
    "test_filepath = download_data(url, 'conllpp_test.txt', dir_name, 748737)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d777ac2",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02f5f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ...\n",
      "\tDone\n",
      "Reading data ...\n",
      "\tDone\n",
      "Reading data ...\n",
      "\tDone\n",
      "Train size: 14041\n",
      "Valid size: 3250\n",
      "Test size: 3452\n",
      "\n",
      "Sample data\n",
      "\n",
      "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: LONDON 1996-08-30\n",
      "Labels: ['B-LOC', 'O']\n",
      "\n",
      "\n",
      "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
      "Labels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "\n",
      "\n",
      "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    '''\n",
    "    Read data from a file with given filename\n",
    "    Returns a list of sentences (each sentence a string), \n",
    "    and list of ner labels for each string\n",
    "    '''\n",
    "\n",
    "    print(\"Reading data ...\")\n",
    "    # master lists - Holds sentences (list of tokens), ner_labels (for each token an NER label)\n",
    "    sentences, ner_labels = [], [] \n",
    "    \n",
    "    # Open the file\n",
    "    with open(filename,'r',encoding='latin-1') as f:        \n",
    "        # Read each line\n",
    "        is_sos = True # We record at each line if we are seeing the beginning of a sentence\n",
    "        \n",
    "        # Tokens and labels of a single sentence, flushed when encountered a new one\n",
    "        sentence_tokens = []\n",
    "        sentence_labels = []\n",
    "        i = 0\n",
    "        for row in f:\n",
    "            # If we are seeing an empty line or -DOCSTART- that's a new line\n",
    "            if len(row.strip()) == 0 or row.split(' ')[0] == '-DOCSTART-':\n",
    "                is_sos = False\n",
    "            # Otherwise keep capturing tokens and labels\n",
    "            else:\n",
    "                is_sos = True\n",
    "                token, _, _, ner_label = row.split(' ')\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(ner_label.strip())\n",
    "            \n",
    "            # When we reach the end / or reach the beginning of next\n",
    "            # add the data to the master lists, flush the temporary one\n",
    "            if not is_sos and len(sentence_tokens)>0:\n",
    "                sentences.append(' '.join(sentence_tokens))\n",
    "                ner_labels.append(sentence_labels)\n",
    "                sentence_tokens, sentence_labels = [], []\n",
    "    \n",
    "    print('\\tDone')\n",
    "    return sentences, ner_labels\n",
    "\n",
    "# Train data\n",
    "train_sentences, train_labels = read_data(train_filepath) \n",
    "# Validation data\n",
    "valid_sentences, valid_labels = read_data(dev_filepath) \n",
    "# Test data\n",
    "test_sentences, test_labels = read_data(test_filepath) \n",
    "\n",
    "# Print some stats\n",
    "print('Train size: {}'.format(len(train_labels)))\n",
    "print('Valid size: {}'.format(len(valid_labels)))\n",
    "print('Test size: {}'.format(len(test_labels)))\n",
    "\n",
    "# Print some data\n",
    "print('\\nSample data\\n')\n",
    "for v_sent, v_labels in zip(valid_sentences[:5], valid_labels[:5]):\n",
    "    print(\"Sentence: {}\".format(v_sent))\n",
    "    print(\"Labels: {}\".format(v_labels))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53895ded",
   "metadata": {},
   "source": [
    "## Checking the balance of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0602956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label counts\n",
      "O         169578\n",
      "B-LOC       7140\n",
      "B-PER       6600\n",
      "B-ORG       6321\n",
      "I-PER       4528\n",
      "I-ORG       3704\n",
      "B-MISC      3438\n",
      "I-LOC       1157\n",
      "I-MISC      1155\n",
      "dtype: int64\n",
      "\n",
      "Validation data label counts\n",
      "O         42759\n",
      "B-PER      1842\n",
      "B-LOC      1837\n",
      "B-ORG      1341\n",
      "I-PER      1307\n",
      "B-MISC      922\n",
      "I-ORG       751\n",
      "I-MISC      346\n",
      "I-LOC       257\n",
      "dtype: int64\n",
      "\n",
      "Test data label counts\n",
      "O         38143\n",
      "B-ORG      1714\n",
      "B-LOC      1645\n",
      "B-PER      1617\n",
      "I-PER      1161\n",
      "I-ORG       881\n",
      "B-MISC      722\n",
      "I-LOC       259\n",
      "I-MISC      252\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Print the value count for each label\n",
    "print(\"Training data label counts\")\n",
    "print(pd.Series(chain(*train_labels)).value_counts())\n",
    "\n",
    "print(\"\\nValidation data label counts\")\n",
    "print(pd.Series(chain(*valid_labels)).value_counts())\n",
    "\n",
    "print(\"\\nTest data label counts\")\n",
    "print(pd.Series(chain(*test_labels)).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47c16e",
   "metadata": {},
   "source": [
    "## Analysing the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a793c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14041.000000\n",
       "mean        14.501887\n",
       "std         11.602756\n",
       "min          1.000000\n",
       "5%           2.000000\n",
       "50%         10.000000\n",
       "95%         37.000000\n",
       "max        113.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_sentences).str.split().str.len().describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971a00a",
   "metadata": {},
   "source": [
    "## Padding/Truncating sentences to create arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f68a579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "{1: 0.006811025015037328, 5: 0.16176470588235295, 3: 0.17500000000000002, 0: 0.18272425249169436, 4: 0.25507950530035334, 6: 0.31182505399568033, 2: 0.33595113438045376, 8: 0.9982713915298186, 7: 1.0}\n",
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "[[0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]\n",
      " [3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]]\n",
      "[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "max_seq_length = 40\n",
    "\n",
    "def get_label_id_map(train_labels):\n",
    "    # Get the unique list of labels\n",
    "    unique_train_labels = pd.Series(chain(*train_labels)).unique()\n",
    "    # Create a class label -> class ID mapping\n",
    "    labels_map = dict(zip(unique_train_labels, np.arange(unique_train_labels.shape[0])))\n",
    "    print(\"labels_map: {}\".format(labels_map))\n",
    "    return labels_map\n",
    "\n",
    "\n",
    "def get_class_weights(train_labels):\n",
    "    \n",
    "    label_count_ser = pd.Series(chain(*train_labels)).value_counts()\n",
    "    label_count_ser = label_count_ser.sum()/label_count_ser\n",
    "    label_count_ser /= label_count_ser.max()\n",
    "    \n",
    "    label_id_map = get_label_id_map(train_labels)\n",
    "    label_count_ser.index = label_count_ser.index.map(label_id_map)\n",
    "    return label_count_ser.to_dict()\n",
    "\n",
    "\n",
    "def get_padded_int_labels(labels, labels_map, max_seq_length, return_mask=True):\n",
    "    \n",
    "    # Create a partial function with many of arguments fixed\n",
    "    # Pad to/Truncate at max_seq_length\n",
    "    ner_pad_sequence_func = partial(\n",
    "        tf.keras.preprocessing.sequence.pad_sequences, maxlen=max_seq_length,\n",
    "        padding='post', truncating='post', \n",
    "    )\n",
    "\n",
    "    # Convert string labels to integers \n",
    "    int_labels = [[labels_map[x] for x in one_seq] for one_seq in labels]\n",
    "    \n",
    "    \n",
    "    # Pad sequences\n",
    "    if return_mask:\n",
    "        # If we return mask, we first pad with a special value (-1) and \n",
    "        # use that to create the mask and later replace -1 with 'O'\n",
    "        padded_labels = np.array(ner_pad_sequence_func(int_labels, value=-1))\n",
    "        \n",
    "        # mask filter\n",
    "        mask_filter = (padded_labels != -1)\n",
    "        # replace -1 with 'O' s ID\n",
    "        padded_labels[~mask_filter] = labels_map['O']        \n",
    "        return padded_labels, mask_filter.astype('int')\n",
    "    \n",
    "    else:\n",
    "        padded_labels = np.array(ner_pad_sequence_func(int_labels, value=labels_map['O']))\n",
    "        return padded_labels\n",
    "    \n",
    "train_class_weights = get_class_weights(train_labels)\n",
    "print(train_class_weights)\n",
    "\n",
    "labels_map = get_label_id_map(train_labels)\n",
    "\n",
    "# Convert string labels to integers for all train/validation/test data\n",
    "# Pad train/validation/test data\n",
    "padded_train_labels, train_mask = get_padded_int_labels(\n",
    "    train_labels, labels_map, max_seq_length, return_mask=True\n",
    ")\n",
    "padded_valid_labels, valid_mask = get_padded_int_labels(\n",
    "    valid_labels, labels_map, max_seq_length, return_mask=True\n",
    ")\n",
    "padded_test_labels, test_mask  = get_padded_int_labels(\n",
    "    test_labels, labels_map, max_seq_length, return_mask=True\n",
    ")\n",
    "\n",
    "\n",
    "# Print some labels IDs\n",
    "print(padded_train_labels[:2])\n",
    "print(train_mask[:2])\n",
    "\n",
    "print(padded_train_labels.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45477734",
   "metadata": {},
   "source": [
    "## Implement a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48d7f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "def get_fitted_token_vectorization_layer(corpus, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "        max_tokens=vocabulary_size, standardize=None,        \n",
    "        output_sequence_length=max_seq_length, \n",
    "    )\n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(corpus)\n",
    "    \n",
    "    # Get the vocabulary size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "\n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "# Input layer\n",
    "word_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# Text vectorize layer\n",
    "vectorize_layer, n_vocab = get_fitted_token_vectorization_layer(train_sentences)\n",
    "\n",
    "# Vectorized output (each word mapped to an int ID)\n",
    "vectorized_out = vectorize_layer(word_input)\n",
    "\n",
    "# Look up embeddings for the returned IDs\n",
    "embedding_layer = layers.Embedding(input_dim=n_vocab, output_dim=64, mask_zero=True)(vectorized_out)\n",
    "\n",
    "# Define a simple RNN layer, it returns an output at each position\n",
    "rnn_layer = layers.SimpleRNN(\n",
    "    units=64, activation='tanh', use_bias=True, return_sequences=True\n",
    ")\n",
    "\n",
    "rnn_out = rnn_layer(embedding_layer)\n",
    "\n",
    "dense_layer = layers.Dense(9, activation='softmax')\n",
    "dense_out = dense_layer(rnn_out)\n",
    "\n",
    "model = tf.keras.Model(inputs=word_input, outputs=dense_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89630d",
   "metadata": {},
   "source": [
    "## Defining a custom metric and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f511c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 40, 64)            1512000   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 40, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 40, 9)             585       \n",
      "=================================================================\n",
      "Total params: 1,520,841\n",
      "Trainable params: 1,520,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def macro_accuracy(y_true, y_pred):\n",
    "    \n",
    "    # [batch size * time]\n",
    "    y_true = tf.cast(tf.reshape(y_true, [-1]), 'int32')\n",
    "    y_pred = tf.cast(tf.reshape(tf.argmax(y_pred, axis=-1), [-1]), 'int32')\n",
    "    \n",
    "    sorted_y_true = tf.sort(y_true)\n",
    "    sorted_inds = tf.argsort(y_true)\n",
    "    \n",
    "    sorted_y_pred = tf.gather(y_pred, sorted_inds)\n",
    "    \n",
    "    sorted_correct = tf.cast(tf.math.equal(sorted_y_true, sorted_y_pred), 'int32')\n",
    "    \n",
    "    # We are adding one to make sure ther eare no division by zero\n",
    "    correct_for_each_label = tf.cast(tf.math.segment_sum(sorted_correct, sorted_y_true), 'float32') + 1\n",
    "    all_for_each_label = tf.cast(tf.math.segment_sum(tf.ones_like(sorted_y_true), sorted_y_true), 'float32') + 1\n",
    "    \n",
    "    mean_accuracy = tf.reduce_mean(correct_for_each_label/all_for_each_label)\n",
    "    \n",
    "    return mean_accuracy\n",
    "        \n",
    "mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name='macro_accuracy')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[mean_accuracy_metric])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a410a",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "When training the model we will use `sample_weight` to counteract class-imbalance. We will not use `class_weight`. When using `class_weight` as follows,\n",
    "\n",
    "```\n",
    "model.fit(\n",
    "        train_sentences[i:i+1], padded_train_labels[i:i+1], \n",
    "        class_weight=train_class_weights,\n",
    "        batch_size=64,\n",
    "        epochs=3, \n",
    "        validation_data=(np.array(valid_sentences), padded_valid_labels)\n",
    ")\n",
    "```\n",
    "\n",
    "it leads to the error below,\n",
    "\n",
    "```\n",
    "InvalidArgumentError: 2 root error(s) found.\n",
    "  (0) Invalid argument:  indices[0] = 11 is not in [0, 9)\n",
    "\t [[{{node GatherV2}}]]\n",
    "\t [[IteratorGetNext]]\n",
    "  (1) Invalid argument:  indices[0] = 11 is not in [0, 9)\n",
    "\t [[{{node GatherV2}}]]\n",
    "\t [[IteratorGetNext]]\n",
    "\t [[model/text_vectorization/cond/then/_0/model/text_vectorization/cond/Pad/_56]]\n",
    "0 successful operations.\n",
    "0 derived errors ignored. [Op:__inference_train_function_7453]\n",
    "\n",
    "Function call stack:\n",
    "train_function -> train_function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4b5988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "220/220 [==============================] - 8s 30ms/step - loss: 0.0297 - macro_accuracy: 0.5642 - val_loss: 0.4387 - val_macro_accuracy: 0.7068\n",
      "Epoch 2/3\n",
      "220/220 [==============================] - 6s 28ms/step - loss: 0.0093 - macro_accuracy: 0.8835 - val_loss: 0.2008 - val_macro_accuracy: 0.7983\n",
      "Epoch 3/3\n",
      "220/220 [==============================] - 6s 28ms/step - loss: 0.0033 - macro_accuracy: 0.9598 - val_loss: 0.1041 - val_macro_accuracy: 0.8047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24cbdfebcc0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_sample_weights_from_class_weights(labels, class_weights):\n",
    "    \"\"\" From the class weights generate sample weights \"\"\"\n",
    "    return np.vectorize(class_weights.get)(labels)\n",
    "\n",
    "\n",
    "# Make train_sequences an array\n",
    "train_sentences = np.array(train_sentences)\n",
    "# Get sample weights (we cannot use class_weight with TextVectorization layer)\n",
    "train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)\n",
    "\n",
    "# Training the model\n",
    "model.fit(\n",
    "        train_sentences, padded_train_labels, \n",
    "        sample_weight=train_sample_weights,\n",
    "        batch_size=64,\n",
    "        epochs=3, \n",
    "        validation_data=(np.array(valid_sentences), padded_valid_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c57948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1143 - macro_accuracy: 0.7732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11428217589855194, 0.7732274532318115]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(test_sentences), padded_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030ff12",
   "metadata": {},
   "source": [
    "## Defining an advance RNN model\n",
    "\n",
    "* Token embeddings + Char embeddings\n",
    "* Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c615473",
   "metadata": {},
   "source": [
    "### Statistics about token lenghts (for char embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc588abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23623.000000\n",
       "mean         6.832705\n",
       "std          2.749288\n",
       "min          1.000000\n",
       "5%           3.000000\n",
       "50%          7.000000\n",
       "95%         12.000000\n",
       "max         61.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ser = pd.Series(pd.Series(train_sentences).str.split().explode().unique())\n",
    "vocab_ser.str.len().describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bf9db",
   "metadata": {},
   "source": [
    "## Testing `TextVectorization` for char level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c07fc70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequence: [[['aaaa'], ['bb'], ['c']], [['d'], ['eee'], ['']]]\n",
      "Vectorized output: [[[2 2 2 2]\n",
      "  [4 4 0 0]\n",
      "  [6 0 0 0]]\n",
      "\n",
      " [[5 0 0 0]\n",
      "  [3 3 3 0]\n",
      "  [0 0 0 0]]]\n",
      "Vocabulary: ['', '[UNK]', 'a', 'e', 'b', 'd', 'c']\n"
     ]
    }
   ],
   "source": [
    "def split_char(token):\n",
    "    \"\"\" Instead of splitting word by word, split each char\"\"\"\n",
    "    return tf.strings.bytes_split(token)\n",
    "\n",
    "\n",
    "# Define a vectorization layer that splits chars\n",
    "vectorization_layer = TextVectorization(\n",
    "        standardize=None,      \n",
    "        split=split_char,\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_corpus_for_char_embeddings(tokenized_sentences, max_seq_length):\n",
    "    \"\"\" Pads each sequence to a maximum length \"\"\"\n",
    "    proc_sentences = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        if len(tokens) >= max_seq_length:\n",
    "            proc_sentences.append([[t] for t in tokens[:max_seq_length]])\n",
    "        else:\n",
    "            proc_sentences.append([[t] for t in tokens+['']*(max_seq_length-len(tokens))])\n",
    "            \n",
    "    return proc_sentences\n",
    "\n",
    "# Define sample data\n",
    "data = ['aaaa bb c', 'd eee']\n",
    "# Pad sequences\n",
    "tokenized_sentences = prepare_corpus_for_char_embeddings([d.split() for d in data], 3)\n",
    "print(\"Padded sequence: {}\".format(tokenized_sentences))\n",
    "\n",
    "# Fit it on a corpus of data\n",
    "vectorization_layer.adapt(tokenized_sentences)\n",
    "\n",
    "# Print data\n",
    "print(\"Vectorized output: {}\".format(vectorization_layer(tokenized_sentences)))\n",
    "print(\"Vocabulary: {}\".format(vectorization_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc75515",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da8abc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 40, 1)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_1 (TextVecto (None, None, 12)     0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 12, 32) 2752        text_vectorization_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization (TextVectori (None, 40)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 12, 1)  161         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 40, 64)       1512000     text_vectorization[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 12)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 40, 76)       0           embedding[0][0]                  \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 40, 128)      18048       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 40, 9)        1161        bidirectional[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,534,122\n",
      "Trainable params: 1,534,122\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "K.clear_session()\n",
    "max_seq_length = 40\n",
    "max_token_length = 12\n",
    "\n",
    "def get_fitted_token_vectorization_layer(corpus, max_seq_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "        max_tokens=vocabulary_size, standardize=None,        \n",
    "        output_sequence_length=max_seq_length, \n",
    "    )\n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(corpus)\n",
    "    \n",
    "    # Get the vocabulary size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "\n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "def get_fitted_char_vectorization_layer(corpus, max_seq_length, max_token_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    def split_char(token):\n",
    "        return tf.strings.bytes_split(token)\n",
    "\n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "        standardize=None,      \n",
    "        split=split_char,\n",
    "        output_sequence_length=max_token_length, \n",
    "    )\n",
    "\n",
    "    tokenized_sentences = [sent.split() for sent in corpus]\n",
    "    padded_tokenized_sentences = prepare_corpus_for_char_embeddings(tokenized_sentences, max_seq_length)\n",
    "    \n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(padded_tokenized_sentences)\n",
    "    \n",
    "    # Get the vocabulary size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "\n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "# Input layer (tokens)\n",
    "word_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# Text vectorize layer (token)\n",
    "token_vectorize_layer, n_token_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length)\n",
    "# Text vectorize layer (char)\n",
    "char_vectorize_layer, n_char_vocab = get_fitted_char_vectorization_layer(train_sentences, max_seq_length, max_token_length)\n",
    "\n",
    "# Vectorized output (each word mapped to an int ID)\n",
    "token_vectorized_out = token_vectorize_layer(word_input)\n",
    "\n",
    "\n",
    "# Vectorized output of each token\n",
    "# Need a [batch size, seq len, 1]\n",
    "# strings.split() returns a RaggedTensor. It needs to be converted to a Tensor. Otherwise the following error will be raised\n",
    "# InvalidArgumentError:  assertion failed: [the given axis (axis = 2) is not squeezable!]\n",
    "#\t [[node model/text_vectorization_1/RaggedSqueeze/Assert/Assert (defined at <ipython-input-26-a2f55ee22434>:17) ]] [Op:__inference_train_function_72435]\n",
    "tokenized_word_input = layers.Lambda(\n",
    "    lambda x: tf.strings.split(x).to_tensor(default_value='', shape=[None, max_seq_length, 1])\n",
    ")(word_input)\n",
    "char_vectorized_out = char_vectorize_layer(tokenized_word_input)\n",
    "\n",
    "# Look up embeddings for the returned IDs\n",
    "token_embedding_out = layers.Embedding(input_dim=n_token_vocab, output_dim=64, mask_zero=True)(token_vectorized_out)\n",
    "\n",
    "# Produces a [batch size, seq length, token_length, emb size]\n",
    "char_embedding_layer = layers.Embedding(input_dim=n_char_vocab, output_dim=32, mask_zero=True)(char_vectorized_out)\n",
    "\n",
    "# A 1D convolutional layer that will generate token embeddings by shifting a convolutional kernel over \n",
    "# the sequence of chars in each token (padded)\n",
    "cnn_output = layers.Conv1D(filters=1, kernel_size=5, strides=1, padding='same', activation='relu')(char_embedding_layer)\n",
    "# There is an additional dimension of size 1 (out channel dimension) that we need to remove\n",
    "cnn_output = layers.Lambda(lambda x: x[:, :, :, 0])(cnn_output)\n",
    "\n",
    "# Concatenate the token and char embeddings\n",
    "concat_embedding_out = layers.Concatenate()([token_embedding_out, cnn_output])\n",
    "\n",
    "# Define a simple bidirectional RNN layer, it returns an output at each position\n",
    "rnn_layer_1 = layers.Bidirectional(layers.SimpleRNN(\n",
    "    units=64, activation='tanh', use_bias=True, return_sequences=True\n",
    "))\n",
    "\n",
    "rnn_out_1 = rnn_layer_1(concat_embedding_out)\n",
    "\n",
    "# Defines the final prediction layer\n",
    "dense_layer = layers.Dense(9, activation='softmax')\n",
    "dense_out = dense_layer(rnn_out_1)\n",
    "\n",
    "# Defines the model\n",
    "char_token_embedding_rnn = tf.keras.Model(inputs=word_input, outputs=dense_out)\n",
    " \n",
    "# Define a macro accuracy measure\n",
    "mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name='macro_accuracy')\n",
    "\n",
    "# Compile the model with a loss optimizer and metrics\n",
    "char_token_embedding_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[mean_accuracy_metric])\n",
    "\n",
    "# Summary of the model\n",
    "char_token_embedding_rnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287aa88",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34951b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "220/220 [==============================] - 15s 57ms/step - loss: 0.0278 - macro_accuracy: 0.5362 - val_loss: 0.4932 - val_macro_accuracy: 0.6981\n",
      "Epoch 2/3\n",
      "220/220 [==============================] - 13s 58ms/step - loss: 0.0074 - macro_accuracy: 0.9156 - val_loss: 0.1565 - val_macro_accuracy: 0.8497\n",
      "Epoch 3/3\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 0.0019 - macro_accuracy: 0.9842 - val_loss: 0.0956 - val_macro_accuracy: 0.8580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24cc713ca90>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sample_weights_from_class_weights(labels, class_weights):\n",
    "    \"\"\" From the class weights generate sample weights \"\"\"\n",
    "    return np.vectorize(class_weights.get)(labels)\n",
    "\n",
    "\n",
    "# Make train_sequences an array\n",
    "train_sentences = np.array(train_sentences)\n",
    "# Get sample weights (we cannot use class_weight with TextVectorization layer)\n",
    "train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)\n",
    "\n",
    "# Training the model\n",
    "char_token_embedding_rnn.fit(\n",
    "    train_sentences, padded_train_labels,\n",
    "    sample_weight=train_sample_weights,\n",
    "    batch_size=64,\n",
    "    epochs=3, \n",
    "    validation_data=(np.array(valid_sentences), padded_valid_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c9f3a3",
   "metadata": {},
   "source": [
    "## Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a61caee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 9ms/step - loss: 0.1050 - macro_accuracy: 0.8327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1049954742193222, 0.8327016830444336]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_token_embedding_rnn.evaluate(np.array(test_sentences), padded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae364ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
