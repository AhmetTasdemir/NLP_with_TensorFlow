{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe: Global Vectors for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.sparse import lil_matrix\n",
    "import nltk # standard preprocessing\n",
    "import operator # sorting items in dictionary by value\n",
    "#nltk.download() #tokenizers/punkt/PY3/english.pickle\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "\n",
    "### Downloading the data\n",
    "\n",
    "This code downloads a [BBC dataset](hhttp://mlg.ucd.ie/files/datasets/bbc-fulltext.zip) consisting of news articles published by BBC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "  \n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "  \n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    if not os.path.exists(extract_path):\n",
    "        \n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "  \n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "    \n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data without Preprocessing \n",
    "\n",
    "Reads data as it is to a string and tokenize it using spaces and returns a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 401.txt\n",
      "Detected 2225 stories\n",
      "865163 words found in the total news set\n",
      "Example words (start):  Ad sales boost Time Warner profit  Quarterly profi\n",
      "Example words (end):  Online was the game, ahhhh them was the days ! LOL\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "    \n",
    "    news_stories = []\n",
    "    \n",
    "    print(\"Reading files\")\n",
    "    \n",
    "    i = 0\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        \n",
    "        for fi, f in enumerate(files):\n",
    "            \n",
    "            if 'README' in f:# or 'entertainment' not in root:\n",
    "                continue\n",
    "            \n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
    "                \n",
    "                story = []\n",
    "                \n",
    "                for row in f:\n",
    "                                        \n",
    "                    story.append(row.strip())\n",
    "                    \n",
    "                # If </BODY> tag is encountered, stop capturing\n",
    "                story = ' '.join(story)                        \n",
    "                news_stories.append(story)  \n",
    "                \n",
    "        print('', end='\\r')\n",
    "        \n",
    "    print(\"\\nDetected {} stories\".format(len(news_stories)))\n",
    "    return news_stories\n",
    "                \n",
    "  \n",
    "news_stories = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "print('{} words found in the total news set'.format(sum([len(story.split(' ')) for story in news_stories])))\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' '\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 69215], ('the', 226881), (',', 184013), ('.', 120944), ('of', 116323)]\n",
      "Sample data [1730, 9, 8, 16741, 223, 4, 5169, 4509, 26, 11641]\n"
     ]
    }
   ],
   "source": [
    "generate_cooc = False\n",
    "def generate_cooc_matrix(text, tokenizer, window_size, n_vocab, use_weighting=True):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    \n",
    "    cooc_mat = lil_matrix((n_vocab, n_vocab), dtype=np.float32)\n",
    "    for sequence in sequences:\n",
    "        for i, wi in zip(np.arange(window_size, len(sequence)-window_size), sequence[window_size:-window_size]):\n",
    "            context_window = sequence[i-window_size: i+window_size+1]\n",
    "            distances = np.abs(np.arange(-window_size, window_size+1))\n",
    "            distances[window_size] = 1.0\n",
    "            nom = np.ones(shape=(window_size*2 + 1,), dtype=np.float32)\n",
    "            nom[window_size] = 0.0\n",
    "\n",
    "            if use_weighting:\n",
    "                cooc_mat[wi, context_window] += nom/distances    # Update element\n",
    "            else:\n",
    "                cooc_mat[wi, context_window] += nom\n",
    "    \n",
    "    return cooc_mat    \n",
    "\n",
    "if generate_cooc:\n",
    "    cooc_mat = generate_cooc_matrix(docs, tokenizer, 4, v_size, True)\n",
    "    save_npz(os.path.join('datasets','cooc_mat.npz'), cooc_mat.tocsr())\n",
    "else:\n",
    "    cooc_mat = load_npz(os.path.join('datasets','cooc_mat.npz')).tolil()\n",
    "    print('Cooc matrix of type {} was loaded from disk'.format(type(cooc_mat).__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: ['a', 'a', 'a', 'a', 'concerted', 'concerted', 'concerted', 'concerted']\n",
      "    labels: ['propaganda', 'is', 'concerted', 'set', 'is', 'a', 'set', 'of']\n",
      "    weights: [0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5]\n",
      "\n",
      "with window_size = 4:\n",
      "    batch: ['set', 'set', 'set', 'set', 'set', 'set', 'set', 'set']\n",
      "    labels: ['propaganda', 'is', 'a', 'concerted', 'of', 'messages', 'aimed', 'at']\n",
      "    weights: [0.25, 0.33333334, 0.5, 1.0, 1.0, 0.5, 0.33333334, 0.25]\n"
     ]
    }
   ],
   "source": [
    "word = 'cat'\n",
    "assert word in tokenizer.word_index, 'Word {} is not in the tokenizer'.format(word)\n",
    "assert tokenizer.word_index[word] <= v_size, 'The word {} is an out of vocabuary word. Please try something else'.format(word)\n",
    "\n",
    "rev_word_index = dict(zip(tokenizer.word_index.values(), tokenizer.word_index.keys()))\n",
    "\n",
    "cooc_vec = np.array(cooc_mat.getrow(tokenizer.word_index[word]).todense()).ravel()\n",
    "max_ind = np.argsort(cooc_vec)[-25:]\n",
    "#print(max_ind)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.bar(np.arange(0, 25), cooc_vec[max_ind])\n",
    "plt.xticks(ticks=np.arange(0, 25), labels=[rev_word_index[i] for i in max_ind], rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_model(v_size):\n",
    "    \n",
    "    w_i = Input(shape=(1,))\n",
    "    w_j = Input(shape=(1,))\n",
    "\n",
    "    emb_i = Flatten()(Embedding(v_size, 96, input_length=1)(w_i))\n",
    "    emb_j = Flatten()(Embedding(v_size, 96, input_length=1)(w_j))\n",
    "\n",
    "    ij_dot = Dot(axes=-1)([emb_i,emb_j])\n",
    "    \n",
    "    b_i = Flatten()(\n",
    "        Embedding(v_size, 1, input_length=1)(w_i)\n",
    "    )\n",
    "    b_j = Flatten()(\n",
    "        Embedding(v_size, 1, input_length=1)(w_j)\n",
    "    )\n",
    "\n",
    "    pred = Add()([ij_dot, b_i, b_j])\n",
    "\n",
    "    def glove_loss(y_true, y_pred):\n",
    "        return K.sum(\n",
    "            K.pow((y_true-1)/100.0, 0.75)*K.square(y_pred - K.log(y_true))\n",
    "        )\n",
    "\n",
    "    model = Model(inputs=[w_i, w_j],outputs=pred)\n",
    "    model.compile(loss=glove_loss, optimizer =Adam(lr=0.0001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = create_glove_model(v_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_mat = load_npz(os.path.join('datasets','cooc_mat.npz'))\n",
    "batch_size =128\n",
    "copy_docs = list(docs)\n",
    "index2word = dict(zip(tokenizer.word_index.values(), tokenizer.word_index.keys()))\n",
    "\"\"\" Each epoch \"\"\"\n",
    "for ep in range(10):\n",
    "    \n",
    "    #valid_words = get_valid_words(docs, 20, tokenizer)\n",
    "    \n",
    "    random.shuffle(copy_docs)\n",
    "    losses = []\n",
    "    \"\"\" Each document (i.e. movie plot) \"\"\"\n",
    "    for doc in copy_docs:\n",
    "        \n",
    "        seq = tokenizer.texts_to_sequences([doc])[0]\n",
    "\n",
    "        \"\"\" Getting skip-gram data \"\"\"\n",
    "        # Negative samples are automatically sampled by tf loss function\n",
    "        wpairs, labels = skipgrams(\n",
    "            sequence=seq, vocabulary_size=v_size, negative_samples=0.0, shuffle=True\n",
    "        )\n",
    "        \n",
    "        if len(wpairs)==0:\n",
    "            continue\n",
    "\n",
    "        sg_in, sg_out = zip(*wpairs)\n",
    "        sg_in, sg_out = np.array(sg_in).reshape(-1,1), np.array(sg_out).reshape(-1,1)\n",
    "        x_ij = np.array(cooc_mat[sg_in[:,0], sg_out[:,0]]).reshape(-1,1) + 1\n",
    "        \n",
    "        assert np.all(np.array(labels)==1)\n",
    "        assert x_ij.shape[0] == sg_in.shape[0], 'X_ij {} shape does not sg_in {}'.format(x_ij.shape, sg_in.shape)\n",
    "        \"\"\" For each batch in the dataset \"\"\"\n",
    "        model.fit([sg_in, sg_out], x_ij, batch_size = batch_size, epochs=1, verbose=0)\n",
    "        l = model.evaluate([sg_in, sg_out], x_ij, batch_size=batch_size, verbose=0)\n",
    "        losses.append(l)\n",
    "    print('Loss in epoch {}: {}'.format(ep, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Word Co-Occurance Matrix\n",
    "Why GloVe shine above context window based method is that it employs global statistics of the corpus in to the model (according to authors). This is done by using information from the word co-occurance matrix to optimize the word vectors. Basically, the X(i,j) entry of the co-occurance matrix says how frequent word i to appear near j. We also use a weighting mechanishm to give more weight to words close together than to ones further-apart (from experiments section of the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Defining Hyperparameters\n",
    "\n",
    "Here we define several hyperparameters including `batch_size` (amount of samples in a single batch) `embedding_size` (size of embedding vectors) `window_size` (context window size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # Data points in a single batch\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "window_size = 4 # How many words to consider left and right.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 50\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
    "\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "\n",
    "epsilon = 1 # used for the stability of log in the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Inputs and Outputs\n",
    "\n",
    "Here we define placeholders for feeding in training inputs and outputs (each of size `batch_size`) and a constant tensor to contain validation examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model Parameters and Other Variables\n",
    "We now define four TensorFlow variables which is composed of an embedding layer, a bias for each input and output words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Computations\n",
    "\n",
    "We first defing a lookup function to fetch the corresponding embedding vectors for a set of given inputs. Then we define a placeholder that takes in the weights for a given batch of data points (`weights_x`) and co-occurence matrix weights (`x_ij`). `weights_x` measures the importance of a data point with respect to how much those two words co-occur and `x_ij` denotes the co-occurence matrix value for the row and column denoted by the words in a datapoint. With these defined, we can define the loss as shown below. For exact details refer Chapter 4 text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Word Similarities \n",
    "We calculate the similarity between two given words in terms of the cosine distance. To do this efficiently we use matrix operations to do so, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter Optimizer\n",
    "\n",
    "We then define a constant learning rate and an optimizer which uses the Adagrad method. Feel free to experiment with other optimizers listed [here](https://www.tensorflow.org/api_guides/python/train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the GloVe Algorithm\n",
    "\n",
    "Here we run the GloVe algorithm we defined above. Specifically, we first initialize variables, and then train the algorithm for many steps (`num_steps`). And every few steps we evaluate the algorithm on a fixed validation set and print out the words that appear to be closest for a given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
