{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6647d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n",
      "env: TFHUB_CACHE_DIR=./tfhub_modules\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "# Making sure we cache the models and are not downloaded all the time\n",
    "%env TFHUB_CACHE_DIR=./tfhub_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08272665",
   "metadata": {},
   "source": [
    "## Using pre-trained ELMo Model\n",
    "\n",
    "### Downloading the ELMo Model from TFHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c5a1d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Remove any ongoing sessions\n",
    "K.clear_session()\n",
    "\n",
    "# Download the ELMo model and save to disk\n",
    "elmo_layer = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\", signature=\"tokens\",signature_outputs_as_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e46bcf",
   "metadata": {},
   "source": [
    "### Formatting the input for ELMo\n",
    "\n",
    "ELMo expects the inputs to be in a specific format. Here we write a function to get the input in that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0f0cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': <tf.Tensor: shape=(2, 6), dtype=string, numpy=\n",
      "array([[b'the', b'cat', b'sat', b'on', b'the', b'mat'],\n",
      "       [b'the', b'mat', b'sat', b'', b'', b'']], dtype=object)>, 'sequence_len': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 3], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "def format_text_for_elmo(texts, lower=True, split=\" \", max_len=None):\n",
    "    \n",
    "    \"\"\" Formats a given text for the ELMo model (takes in a list of strings) \"\"\"\n",
    "        \n",
    "    token_inputs = [] # Maintains individual tokens\n",
    "    token_lengths = [] # Maintains the length of each sequence\n",
    "    \n",
    "    max_len_inferred = 0 # We keep a variable to matain the max length of the input\n",
    "    \n",
    "    # Go through each text (string)\n",
    "    for text in texts:    \n",
    "        \n",
    "        # Process the text and get a list of tokens\n",
    "        tokens = tf.keras.preprocessing.text.text_to_word_sequence(text, lower=lower, split=split)\n",
    "        \n",
    "        # Add the tokens \n",
    "        token_inputs.append(tokens)                   \n",
    "        \n",
    "        # Compute the max length for the collection of sequences\n",
    "        if len(tokens)>max_len_inferred:\n",
    "            max_len_inferred = len(tokens)\n",
    "    \n",
    "    # It's important to make sure the maximum token length is only as large as the longest input in the sequence\n",
    "    # You can't have arbitrarily large length as the maximum length. Otherwise, you'll get this error.\n",
    "    #InvalidArgumentError:  Incompatible shapes: [2,6,1] vs. [2,10,1024]\n",
    "    #    [[node mul (defined at .../python3.6/site-packages/tensorflow_hub/module_v2.py:106) ]] [Op:__inference_pruned_3391]\n",
    "    \n",
    "    # Here we make sure max_len is only as large as the longest input\n",
    "    if max_len and max_len_inferred < max_len:\n",
    "        max_len = max_len_inferred\n",
    "    if not max_len:\n",
    "        max_len = max_len_inferred\n",
    "    \n",
    "    # Go through each token sequence and modify sequences to have same length\n",
    "    for i, token_seq in enumerate(token_inputs):\n",
    "        \n",
    "        token_lengths.append(min(len(token_seq), max_len))\n",
    "        \n",
    "        # If the maximum length is less than input length, truncate\n",
    "        if max_len < len(token_seq):\n",
    "            token_seq = token_seq[:max_len]            \n",
    "        # If the maximum length is greater than or equal to input length, add padding as needed\n",
    "        else:            \n",
    "            token_seq = token_seq+[\"\"]*(max_len-len(inp))\n",
    "                \n",
    "        assert len(token_seq)==max_len\n",
    "        \n",
    "        token_inputs[i] = token_seq\n",
    "    \n",
    "    # Return the final output\n",
    "    return {\n",
    "        \"tokens\": tf.constant(token_inputs), \n",
    "        \"sequence_len\": tf.constant(token_lengths)\n",
    "    }\n",
    "\n",
    "\n",
    "print(format_text_for_elmo([\"the cat sat on the mat\", \"the mat sat\"], max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45986d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor under key=sequence_len is a (5,) shaped Tensor\n",
      "Tensor under key=elmo is a (5, 6, 1024) shaped Tensor\n",
      "Tensor under key=default is a (5, 1024) shaped Tensor\n",
      "Tensor under key=lstm_outputs1 is a (5, 6, 1024) shaped Tensor\n",
      "Tensor under key=lstm_outputs2 is a (5, 6, 1024) shaped Tensor\n",
      "Tensor under key=word_emb is a (5, 6, 512) shaped Tensor\n"
     ]
    }
   ],
   "source": [
    "# Titles of 001.txt - 005.txt in bbc/business\n",
    "elmo_inputs = format_text_for_elmo([\n",
    "    \"Ad sales boost Time Warner profit\",\n",
    "    \"Dollar gains on Greenspan speech\",\n",
    "    \"Yukos unit buyer faces loan claim\",\n",
    "    \"High fuel prices hit BA's profits\",\n",
    "    \"Pernod takeover talk lifts Domecq\"\n",
    "])\n",
    "\n",
    "# Get the result from ELMo\n",
    "elmo_result = elmo_layer(elmo_inputs)\n",
    "\n",
    "# Print the result\n",
    "for k,v in elmo_result.items():    \n",
    "    print(\"Tensor under key={} is a {} shaped Tensor\".format(k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb329b",
   "metadata": {},
   "source": [
    "## Generating Document Embeddings with ELMo\n",
    "\n",
    "### Downloading the data\n",
    "\n",
    "This code downloads a [BBC dataset](hhttp://mlg.ucd.ie/files/datasets/bbc-fulltext.zip) consisting of news articles published by BBC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801a2bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "bbc-fulltext.zip has already been extracted\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    \n",
    "    # Create the data directory if not exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "    \n",
    "    # If file doesnt exist, download\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "  \n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    \n",
    "    # If data has not been extracted already, extract data\n",
    "    if not os.path.exists(extract_path):        \n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "    \n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416dba6e",
   "metadata": {},
   "source": [
    "### Read Data without Preprocessing \n",
    "\n",
    "Here we read all the files and keep them as a list of strings, where each string is a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b0a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 361.txt\n",
      "Detected 2225 stories\n",
      "865163 words found in the total news set\n",
      "Example words (start):  Windows worm travels with Tetris  Users are being \n",
      "Example words (end):  is years at Stradey as \"the best time of my life.\"\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "    \n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []    \n",
    "    filenames = []\n",
    "    print(\"Reading files\")\n",
    "    \n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        \n",
    "        for fi, f in enumerate(files):\n",
    "            \n",
    "            # We don't read the readme file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "            \n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "            \n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as text_file:\n",
    "                \n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in text_file:\n",
    "                                        \n",
    "                    story.append(row.strip())\n",
    "                    \n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)                        \n",
    "                # Add that to the list\n",
    "                news_stories.append(story)  \n",
    "                filenames.append(os.path.join(root, f))\n",
    "                \n",
    "        print('', end='\\r')\n",
    "        \n",
    "    print(\"\\nDetected {} stories\".format(len(news_stories)))\n",
    "    return news_stories, filenames\n",
    "                \n",
    "  \n",
    "news_stories, filenames = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print('{} words found in the total news set'.format(sum([len(story.split(' ')) for story in news_stories])))\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ff4ec",
   "metadata": {},
   "source": [
    "### Check the length statistics \n",
    "\n",
    "Here we look at the 95-percientile in order to decide a good sequence length for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3337475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2225.000000\n",
       "mean      388.837303\n",
       "std       241.484273\n",
       "min        91.000000\n",
       "5%        164.200000\n",
       "50%       336.000000\n",
       "95%       736.800000\n",
       "max      4489.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series([len(x.split(' ')) for x in news_stories]).describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a8307",
   "metadata": {},
   "source": [
    "### Compute the document embeddings\n",
    "\n",
    "ELMo provides several outputs as the output (in the form of a dictionary). The most important output is in a key called `default` which is the averaged vector resulting from vectors produced for all the tokens in the input. We will use this as the document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d915657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "news_elmo_embeddings = []\n",
    "\n",
    "# Go through batches\n",
    "for i in range(0, len(news_stories), batch_size):\n",
    "    \n",
    "    # Print progress\n",
    "    print('.', end='')\n",
    "    # Format ELMo inputs\n",
    "    elmo_inputs = format_text_for_elmo(news_stories[i: min(i+batch_size, len(news_stories))], max_len=768)    \n",
    "    # Get the result stored in default\n",
    "    elmo_result = elmo_layer(elmo_inputs)[\"default\"]\n",
    "    # Add that to a list\n",
    "    news_elmo_embeddings.append(elmo_result)\n",
    "\n",
    "# Create an array\n",
    "news_elmo_embeddings = np.concatenate(news_elmo_embeddings, axis=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb919f",
   "metadata": {},
   "source": [
    "### Save the embeddings to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aaf38bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to disk\n",
    "os.makedirs('elmo_embeddings', exist_ok=True)\n",
    "\n",
    "pd.DataFrame(\n",
    "    news_elmo_embeddings, index=filenames\n",
    ").to_pickle(\n",
    "    os.path.join('elmo_embeddings', 'elmo_embeddings.pkl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac4aaa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/272.txt</th>\n",
       "      <td>0.144291</td>\n",
       "      <td>0.015962</td>\n",
       "      <td>-0.151633</td>\n",
       "      <td>0.096555</td>\n",
       "      <td>-0.015913</td>\n",
       "      <td>0.075896</td>\n",
       "      <td>-0.033353</td>\n",
       "      <td>-0.118644</td>\n",
       "      <td>-0.192030</td>\n",
       "      <td>-0.124919</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.418126</td>\n",
       "      <td>0.204618</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>0.146415</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>-0.005937</td>\n",
       "      <td>0.377529</td>\n",
       "      <td>-0.071118</td>\n",
       "      <td>0.302366</td>\n",
       "      <td>-0.099605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/127.txt</th>\n",
       "      <td>0.022870</td>\n",
       "      <td>-0.142899</td>\n",
       "      <td>-0.017096</td>\n",
       "      <td>-0.084165</td>\n",
       "      <td>0.320108</td>\n",
       "      <td>0.424914</td>\n",
       "      <td>-0.043930</td>\n",
       "      <td>0.257134</td>\n",
       "      <td>-0.215543</td>\n",
       "      <td>-0.046845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219130</td>\n",
       "      <td>0.264653</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.154055</td>\n",
       "      <td>0.091697</td>\n",
       "      <td>0.081720</td>\n",
       "      <td>0.279901</td>\n",
       "      <td>-0.111844</td>\n",
       "      <td>0.448175</td>\n",
       "      <td>0.007058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/370.txt</th>\n",
       "      <td>0.207623</td>\n",
       "      <td>0.058697</td>\n",
       "      <td>-0.008874</td>\n",
       "      <td>-0.088409</td>\n",
       "      <td>0.193419</td>\n",
       "      <td>0.046109</td>\n",
       "      <td>-0.107221</td>\n",
       "      <td>0.199647</td>\n",
       "      <td>-0.167632</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054829</td>\n",
       "      <td>0.225892</td>\n",
       "      <td>0.052450</td>\n",
       "      <td>0.157943</td>\n",
       "      <td>-0.054407</td>\n",
       "      <td>0.171159</td>\n",
       "      <td>0.299693</td>\n",
       "      <td>0.078852</td>\n",
       "      <td>0.167330</td>\n",
       "      <td>-0.113994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/329.txt</th>\n",
       "      <td>0.022106</td>\n",
       "      <td>0.060943</td>\n",
       "      <td>-0.127390</td>\n",
       "      <td>-0.100214</td>\n",
       "      <td>0.184243</td>\n",
       "      <td>-0.077529</td>\n",
       "      <td>-0.157470</td>\n",
       "      <td>-0.042993</td>\n",
       "      <td>-0.204254</td>\n",
       "      <td>-0.021419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337353</td>\n",
       "      <td>0.153419</td>\n",
       "      <td>0.052486</td>\n",
       "      <td>0.342915</td>\n",
       "      <td>0.268746</td>\n",
       "      <td>0.212994</td>\n",
       "      <td>0.665159</td>\n",
       "      <td>0.119243</td>\n",
       "      <td>0.474922</td>\n",
       "      <td>-0.215598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/240.txt</th>\n",
       "      <td>0.259128</td>\n",
       "      <td>-0.108082</td>\n",
       "      <td>0.076262</td>\n",
       "      <td>-0.080416</td>\n",
       "      <td>0.183988</td>\n",
       "      <td>0.329807</td>\n",
       "      <td>0.156697</td>\n",
       "      <td>0.495652</td>\n",
       "      <td>-0.104913</td>\n",
       "      <td>-0.120077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218093</td>\n",
       "      <td>0.236378</td>\n",
       "      <td>0.076534</td>\n",
       "      <td>0.162548</td>\n",
       "      <td>0.025069</td>\n",
       "      <td>0.169282</td>\n",
       "      <td>0.229194</td>\n",
       "      <td>-0.025068</td>\n",
       "      <td>0.351246</td>\n",
       "      <td>0.069058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/sport/156.txt</th>\n",
       "      <td>-0.129396</td>\n",
       "      <td>-0.318909</td>\n",
       "      <td>0.051668</td>\n",
       "      <td>-0.004754</td>\n",
       "      <td>0.278222</td>\n",
       "      <td>0.212088</td>\n",
       "      <td>0.019598</td>\n",
       "      <td>0.088313</td>\n",
       "      <td>-0.207423</td>\n",
       "      <td>0.119865</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136864</td>\n",
       "      <td>0.209640</td>\n",
       "      <td>-0.043528</td>\n",
       "      <td>-0.028529</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>-0.093324</td>\n",
       "      <td>0.170397</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.681819</td>\n",
       "      <td>-0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/sport/151.txt</th>\n",
       "      <td>-0.189324</td>\n",
       "      <td>-0.460702</td>\n",
       "      <td>0.184651</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.262374</td>\n",
       "      <td>0.318553</td>\n",
       "      <td>0.081744</td>\n",
       "      <td>-0.092602</td>\n",
       "      <td>-0.151647</td>\n",
       "      <td>-0.034163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139452</td>\n",
       "      <td>0.205881</td>\n",
       "      <td>-0.088978</td>\n",
       "      <td>-0.145100</td>\n",
       "      <td>-0.125726</td>\n",
       "      <td>-0.159489</td>\n",
       "      <td>0.113482</td>\n",
       "      <td>-0.020790</td>\n",
       "      <td>0.695474</td>\n",
       "      <td>0.062283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/sport/042.txt</th>\n",
       "      <td>-0.081969</td>\n",
       "      <td>-0.074043</td>\n",
       "      <td>-0.004957</td>\n",
       "      <td>0.078592</td>\n",
       "      <td>0.142221</td>\n",
       "      <td>0.236764</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>-0.033401</td>\n",
       "      <td>-0.140084</td>\n",
       "      <td>-0.310319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048195</td>\n",
       "      <td>0.119734</td>\n",
       "      <td>-0.381953</td>\n",
       "      <td>0.163369</td>\n",
       "      <td>0.149823</td>\n",
       "      <td>-0.129863</td>\n",
       "      <td>-0.020217</td>\n",
       "      <td>-0.316813</td>\n",
       "      <td>0.319503</td>\n",
       "      <td>0.091818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/sport/194.txt</th>\n",
       "      <td>-0.270698</td>\n",
       "      <td>-0.454356</td>\n",
       "      <td>0.062119</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>0.250537</td>\n",
       "      <td>0.327598</td>\n",
       "      <td>-0.133687</td>\n",
       "      <td>0.171560</td>\n",
       "      <td>-0.061138</td>\n",
       "      <td>0.241108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302931</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>-0.093189</td>\n",
       "      <td>-0.233209</td>\n",
       "      <td>-0.016422</td>\n",
       "      <td>0.026764</td>\n",
       "      <td>0.151837</td>\n",
       "      <td>0.066245</td>\n",
       "      <td>0.472599</td>\n",
       "      <td>0.031884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/sport/361.txt</th>\n",
       "      <td>-0.124090</td>\n",
       "      <td>-0.201660</td>\n",
       "      <td>0.036653</td>\n",
       "      <td>-0.004642</td>\n",
       "      <td>0.301912</td>\n",
       "      <td>0.313855</td>\n",
       "      <td>0.107318</td>\n",
       "      <td>0.233952</td>\n",
       "      <td>-0.148053</td>\n",
       "      <td>0.025082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088466</td>\n",
       "      <td>0.202406</td>\n",
       "      <td>0.010581</td>\n",
       "      <td>0.035258</td>\n",
       "      <td>0.092716</td>\n",
       "      <td>-0.024548</td>\n",
       "      <td>0.225901</td>\n",
       "      <td>0.119615</td>\n",
       "      <td>0.565442</td>\n",
       "      <td>-0.053254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0         1         2         3         4     \\\n",
       "data/bbc/tech/272.txt   0.144291  0.015962 -0.151633  0.096555 -0.015913   \n",
       "data/bbc/tech/127.txt   0.022870 -0.142899 -0.017096 -0.084165  0.320108   \n",
       "data/bbc/tech/370.txt   0.207623  0.058697 -0.008874 -0.088409  0.193419   \n",
       "data/bbc/tech/329.txt   0.022106  0.060943 -0.127390 -0.100214  0.184243   \n",
       "data/bbc/tech/240.txt   0.259128 -0.108082  0.076262 -0.080416  0.183988   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "data/bbc/sport/156.txt -0.129396 -0.318909  0.051668 -0.004754  0.278222   \n",
       "data/bbc/sport/151.txt -0.189324 -0.460702  0.184651  0.008714  0.262374   \n",
       "data/bbc/sport/042.txt -0.081969 -0.074043 -0.004957  0.078592  0.142221   \n",
       "data/bbc/sport/194.txt -0.270698 -0.454356  0.062119  0.009040  0.250537   \n",
       "data/bbc/sport/361.txt -0.124090 -0.201660  0.036653 -0.004642  0.301912   \n",
       "\n",
       "                            5         6         7         8         9     ...  \\\n",
       "data/bbc/tech/272.txt   0.075896 -0.033353 -0.118644 -0.192030 -0.124919  ...   \n",
       "data/bbc/tech/127.txt   0.424914 -0.043930  0.257134 -0.215543 -0.046845  ...   \n",
       "data/bbc/tech/370.txt   0.046109 -0.107221  0.199647 -0.167632  0.003790  ...   \n",
       "data/bbc/tech/329.txt  -0.077529 -0.157470 -0.042993 -0.204254 -0.021419  ...   \n",
       "data/bbc/tech/240.txt   0.329807  0.156697  0.495652 -0.104913 -0.120077  ...   \n",
       "...                          ...       ...       ...       ...       ...  ...   \n",
       "data/bbc/sport/156.txt  0.212088  0.019598  0.088313 -0.207423  0.119865  ...   \n",
       "data/bbc/sport/151.txt  0.318553  0.081744 -0.092602 -0.151647 -0.034163  ...   \n",
       "data/bbc/sport/042.txt  0.236764  0.083222 -0.033401 -0.140084 -0.310319  ...   \n",
       "data/bbc/sport/194.txt  0.327598 -0.133687  0.171560 -0.061138  0.241108  ...   \n",
       "data/bbc/sport/361.txt  0.313855  0.107318  0.233952 -0.148053  0.025082  ...   \n",
       "\n",
       "                            1014      1015      1016      1017      1018  \\\n",
       "data/bbc/tech/272.txt  -0.418126  0.204618  0.007118  0.146415  0.010992   \n",
       "data/bbc/tech/127.txt  -0.219130  0.264653  0.020197  0.154055  0.091697   \n",
       "data/bbc/tech/370.txt  -0.054829  0.225892  0.052450  0.157943 -0.054407   \n",
       "data/bbc/tech/329.txt  -0.337353  0.153419  0.052486  0.342915  0.268746   \n",
       "data/bbc/tech/240.txt  -0.218093  0.236378  0.076534  0.162548  0.025069   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "data/bbc/sport/156.txt -0.136864  0.209640 -0.043528 -0.028529  0.012553   \n",
       "data/bbc/sport/151.txt -0.139452  0.205881 -0.088978 -0.145100 -0.125726   \n",
       "data/bbc/sport/042.txt  0.048195  0.119734 -0.381953  0.163369  0.149823   \n",
       "data/bbc/sport/194.txt -0.302931  0.165477 -0.093189 -0.233209 -0.016422   \n",
       "data/bbc/sport/361.txt -0.088466  0.202406  0.010581  0.035258  0.092716   \n",
       "\n",
       "                            1019      1020      1021      1022      1023  \n",
       "data/bbc/tech/272.txt  -0.005937  0.377529 -0.071118  0.302366 -0.099605  \n",
       "data/bbc/tech/127.txt   0.081720  0.279901 -0.111844  0.448175  0.007058  \n",
       "data/bbc/tech/370.txt   0.171159  0.299693  0.078852  0.167330 -0.113994  \n",
       "data/bbc/tech/329.txt   0.212994  0.665159  0.119243  0.474922 -0.215598  \n",
       "data/bbc/tech/240.txt   0.169282  0.229194 -0.025068  0.351246  0.069058  \n",
       "...                          ...       ...       ...       ...       ...  \n",
       "data/bbc/sport/156.txt -0.093324  0.170397  0.002566  0.681819 -0.002700  \n",
       "data/bbc/sport/151.txt -0.159489  0.113482 -0.020790  0.695474  0.062283  \n",
       "data/bbc/sport/042.txt -0.129863 -0.020217 -0.316813  0.319503  0.091818  \n",
       "data/bbc/sport/194.txt  0.026764  0.151837  0.066245  0.472599  0.031884  \n",
       "data/bbc/sport/361.txt -0.024548  0.225901  0.119615  0.565442 -0.053254  \n",
       "\n",
       "[2225 rows x 1024 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(os.path.join('elmo_embeddings', 'elmo_embeddings.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867d719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
